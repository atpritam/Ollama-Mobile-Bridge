"""
Text similarity utilities for fuzzy cache key matching.
Combines multiple similarity metrics to identify semantically similar queries.
"""
import re
from typing import Set, List
import Levenshtein
import jellyfish
import wn
from functools import lru_cache


class TextSimilarity:
    """
    Hybrid text similarity calculator using multiple algorithms.
    Optimized for short search queries generated by LLMs.
    """

    WEIGHTS = {
        'jaccard': 0.37,
        'cosine': 0.37,
        'jaro_winkler': 0.16,
        'levenshtein': 0.10
    }

    # WordNet instance
    _wordnet = None

    STOPWORDS = {
        'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',
        'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',
        'to', 'was', 'will', 'with', 'the', 'this', 'but', 'they', 'have',
        'had', 'what', 'when', 'where', 'who', 'which', 'why', 'how'
    }

    @classmethod
    def _get_wordnet(cls):
        """Lazy load WordNet lexicon."""
        if cls._wordnet is None:
            try:
                cls._wordnet = wn.Wordnet('oewn:2024')
            except Exception:
                try:
                    wn.download('oewn:2024')
                    cls._wordnet = wn.Wordnet('oewn:2024')
                except Exception:
                    cls._wordnet = False
        return cls._wordnet if cls._wordnet is not False else None

    @staticmethod
    def normalize_query(query: str) -> str:
        """Normalize query for better similarity matching."""
        # Lowercase and remove punctuation
        query = query.lower()
        query = re.sub(r'[^\w\s]', ' ', query)
        # Split into words and remove extra whitespace
        words = query.split()
        # Sort words alphabetically
        words.sort()

        return ' '.join(words)

    @staticmethod
    def simhash(text: str, hash_bits: int = 64) -> int:
        """Generate simhash fingerprint for near-duplicate detection."""
        # Generate character n-grams (shingles)
        tokens = TextSimilarity._get_tokens(text)

        if not tokens:
            return 0

        # Vector to accumulate weighted bits
        v = [0] * hash_bits

        for token in tokens:
            h = hash(token)

            # Update vector based on hash bits
            for i in range(hash_bits):
                if h & (1 << i):
                    v[i] += 1
                else:
                    v[i] -= 1

        # Generate final fingerprint
        fingerprint = 0
        for i in range(hash_bits):
            if v[i] > 0:
                fingerprint |= (1 << i)

        return fingerprint

    @staticmethod
    def simhash_distance(hash1: int, hash2: int) -> int:
        """
        Calculate Hamming distance between two simhash values.
        Lower distance = more similar.

        Args:
            hash1: First simhash
            hash2: Second simhash

        Returns:
            Number of differing bits (0-64)
        """
        # XOR gives us bits that differ
        xor = hash1 ^ hash2
        distance = 0
        while xor:
            distance += 1
            xor &= xor - 1

        return distance

    @staticmethod
    def simhash_similarity(hash1: int, hash2: int, hash_bits: int = 64) -> float:
        """Convert simhash Hamming distance to similarity score (0-1)."""
        distance = TextSimilarity.simhash_distance(hash1, hash2)
        return 1.0 - (distance / hash_bits)

    @staticmethod
    def _get_tokens(text: str, n: int = 3) -> Set[str]:
        """
        Generate character n-grams (shingles) from text.

        Args:
            text: Input text
            n: N-gram size

        Returns:
            Set of n-grams
        """
        text = text.lower()
        if len(text) < n:
            return {text}

        return {text[i:i+n] for i in range(len(text) - n + 1)}

    @classmethod
    @lru_cache(maxsize=1000)
    def get_synonyms(cls, word: str, max_synonyms: int = 5) -> Set[str]:
        """
        Get synonyms for a word using WordNet.

        Args:
            word: Input word (lowercase)
            max_synonyms: Maximum number of synonyms to return per word

        Returns:
            Set of synonyms including the original word
        """
        wordnet = cls._get_wordnet()
        if not wordnet or word in cls.STOPWORDS:
            return {word}

        synonyms = {word}

        try:
            synsets = wordnet.synsets(word)

            # Collect synonyms from the first few synsets
            for synset in synsets[:2]:
                # Get lemmas (word forms) from this synset
                for lemma in synset.lemmas()[:max_synonyms]:
                    lemma_text = lemma.lower().replace('_', ' ')
                    if ' ' not in lemma_text:
                        synonyms.add(lemma_text)

                    if len(synonyms) >= max_synonyms + 1:
                        break

                if len(synonyms) >= max_synonyms + 1:
                    break

        except Exception:
            pass

        return synonyms

    @classmethod
    def expand_with_synonyms(cls, text: str, max_synonyms: int = 3) -> Set[str]:
        """
        Expand text with synonyms for better semantic matching.

        Args:
            text: Input text
            max_synonyms: Maximum synonyms per word

        Returns:
            Set of words including originals and their synonyms
        """
        words = set(text.lower().split())
        expanded = set()

        for word in words:
            if len(word) <= 2 or word in cls.STOPWORDS:
                expanded.add(word)
            else:
                synonyms = cls.get_synonyms(word, max_synonyms)
                expanded.update(synonyms)

        return expanded

    @staticmethod
    def jaccard_similarity(text1: str, text2: str) -> float:
        """
        Calculate Jaccard similarity based on word overlap.

        Jaccard = |intersection| / |union|

        Args:
            text1: First text
            text2: Second text

        Returns:
            Similarity score (0-1)
        """
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1 and not words2:
            return 1.0
        if not words1 or not words2:
            return 0.0

        intersection = words1 & words2
        union = words1 | words2

        return len(intersection) / len(union)

    @classmethod
    def jaccard_similarity_with_synonyms(
        cls, text1: str, text2: str, max_synonyms: int = 3
    ) -> float:
        """
        Calculate Jaccard similarity with synonym expansion using WordNet.

        This method expands words with their synonyms before calculating similarity,
        allowing "car" to match "automobile", "big" to match "large", etc.

        Args:
            text1: First text
            text2: Second text
            max_synonyms: Maximum synonyms per word (default: 3)

        Returns:
            Similarity score (0-1)
        """
        # Expand both texts with synonyms
        expanded1 = cls.expand_with_synonyms(text1, max_synonyms)
        expanded2 = cls.expand_with_synonyms(text2, max_synonyms)

        if not expanded1 and not expanded2:
            return 1.0
        if not expanded1 or not expanded2:
            return 0.0

        # Calculate intersection and union with expanded word sets
        intersection = expanded1 & expanded2
        union = expanded1 | expanded2

        return len(intersection) / len(union) if union else 0.0

    @staticmethod
    def cosine_similarity(text1: str, text2: str, n: int = 3) -> float:
        """
        Calculate cosine similarity using character n-grams.

        Args:
            text1: First text
            text2: Second text
            n: N-gram size

        Returns:
            Similarity score (0-1)
        """
        tokens1 = TextSimilarity._get_tokens(text1, n)
        tokens2 = TextSimilarity._get_tokens(text2, n)

        if not tokens1 and not tokens2:
            return 1.0
        if not tokens1 or not tokens2:
            return 0.0

        # Calculate intersection size
        intersection = len(tokens1 & tokens2)

        # Cosine similarity formula
        denominator = (len(tokens1) * len(tokens2)) ** 0.5

        return intersection / denominator if denominator > 0 else 0.0

    @staticmethod
    def jaro_winkler_similarity(s1: str, s2: str, scaling: float = 0.1) -> float:
        """Calculate Jaro-Winkler similarity."""
        return jellyfish.jaro_winkler_similarity(s1, s2)

    @staticmethod
    def levenshtein_similarity(s1: str, s2: str) -> float:
        """Calculate Levenshtein edit distance."""
        max_len = max(len(s1), len(s2))

        if max_len == 0:
            return 1.0

        distance = Levenshtein.distance(s1, s2)
        return 1.0 - (distance / max_len)

    @classmethod
    def hybrid_similarity(
        cls, text1: str, text2: str, use_synonyms: bool = True, max_synonyms: int = 3
    ) -> float:
        """
        Calculate weighted hybrid similarity score using multiple metrics.

        Args:
            text1: First text
            text2: Second text
            use_synonyms: Enable synonym expansion for Jaccard similarity
            max_synonyms: Maximum synonyms per word when use_synonyms=True

        Returns:
            Similarity score (0-1)
        """
        if text1 == text2:
            return 1.0

        # Calculate individual metrics
        if use_synonyms:
            jaccard = cls.jaccard_similarity_with_synonyms(text1, text2, max_synonyms)
        else:
            jaccard = cls.jaccard_similarity(text1, text2)

        cosine = cls.cosine_similarity(text1, text2)
        jaro = cls.jaro_winkler_similarity(text1, text2)
        leven = cls.levenshtein_similarity(text1, text2)

        # Weighted combination
        score = (
            cls.WEIGHTS['jaccard'] * jaccard +
            cls.WEIGHTS['cosine'] * cosine +
            cls.WEIGHTS['jaro_winkler'] * jaro +
            cls.WEIGHTS['levenshtein'] * leven
        )

        return score

    @classmethod
    def find_similar_queries(
        cls,
        new_query: str,
        cached_queries: List[str],
        threshold: float = 0.80,
        use_simhash: bool = True,
        simhash_threshold: int = 10,
        use_synonyms: bool = True,
        max_synonyms: int = 3
    ) -> List[tuple[str, float]]:
        """
        Find cached queries similar to the new query.

        Args:
            new_query: New search query
            cached_queries: List of previously cached queries
            threshold: Minimum similarity score (0-1) for a match
            use_simhash: Whether to use simhash for fast pre-filtering
            simhash_threshold: Maximum simhash distance for candidates

        Returns:
            List of (query, similarity_score) tuples, sorted by score (descending)
        """
        if not cached_queries:
            return []

        # Normalize the new query
        new_normalized = TextSimilarity.normalize_query(new_query)

        # Fast pre-filtering with simhash if enabled
        candidates = cached_queries
        if use_simhash and len(cached_queries) > 10:
            new_hash = TextSimilarity.simhash(new_normalized)
            candidates = []

            for cached_query in cached_queries:
                cached_normalized = TextSimilarity.normalize_query(cached_query)
                cached_hash = TextSimilarity.simhash(cached_normalized)

                # Only consider candidates within simhash distance threshold
                if TextSimilarity.simhash_distance(new_hash, cached_hash) <= simhash_threshold:
                    candidates.append(cached_query)

        # Calculate hybrid similarity for candidates
        matches = []
        for cached_query in candidates:
            cached_normalized = cls.normalize_query(cached_query)
            similarity = cls.hybrid_similarity(
                new_normalized, cached_normalized,
                use_synonyms=use_synonyms,
                max_synonyms=max_synonyms
            )

            if similarity >= threshold:
                matches.append((cached_query, similarity))

        # Sort by similarity score (descending)
        matches.sort(key=lambda x: x[1], reverse=True)

        return matches